{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Installation\n",
    "\n",
    "https://towardsdatascience.com/create-your-first-etl-pipeline-in-apache-spark-and-python-ec3d12e2c169 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparksession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is the entry point for programming Spark applications. It let you interact with DataSet and DataFrame APIs provided by Spark. We set the application name by calling appName. The getOrCreate() method either returns a new SparkSession of the app or returns the existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/14 19:40:31 WARN Utils: Your hostname, Ubi20 resolves to a loopback address: 127.0.1.1; using 192.168.0.5 instead (on interface wlp2s0)\n",
      "22/07/14 19:40:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/14 19:40:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    scSpark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"reading csv\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the file path and then called .read.csv to read the CSV file. The .cache() caches the returned resultset hence increase the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records = 4\n",
      "+------+----+--------+\n",
      "| name |age | country|\n",
      "+------+----+--------+\n",
      "| adnan|  40|Pakistan|\n",
      "|  maaz|   9|Pakistan|\n",
      "|  musa|   4|Pakistan|\n",
      "|ayesha|  32|Pakistan|\n",
      "+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data.csv'\n",
    "sdfData = scSpark.read.csv(data_file, header=True, sep=\",\").cache()\n",
    "print('Total Records = {}'.format(sdfData.count()))\n",
    "sdfData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read multiple files: it will read all files starts with data and of type CSV. it dumps all the data from the CSVs into a single dataframe. this dumping will only work if all the CSVs follow a certain schema, not if you have a CSV with different column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records = 8\n",
      "+-------+----+--------+\n",
      "|  name |age | country|\n",
      "+-------+----+--------+\n",
      "| noreen|  23| England|\n",
      "|  Aamir|   9|Pakistan|\n",
      "|  Noman|   4|Pakistan|\n",
      "|Rasheed|  12|Pakistan|\n",
      "|  adnan|  40|Pakistan|\n",
      "|   maaz|   9|Pakistan|\n",
      "|   musa|   4|Pakistan|\n",
      "| ayesha|  32|Pakistan|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data*.csv'\n",
    "sdfData = scSpark.read.csv(data_file, header=True, sep=\",\").cache()\n",
    "print('Total Records = {}'.format(sdfData.count()))\n",
    "sdfData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform many operations with DataFrame but Spark provides you much easier and familiar interface to manipulate the data by using SQLContext. It is the gateway to SparkSQL which lets you use SQL like queries to get the desired results. Before we try SQL queries, let’s try to group records by Gender. We are dealing with the EXTRACT part of the ETL here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|Female|  501|\n",
      "|  Male|  499|\n",
      "+------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_file = 'supermarket_sales.csv'\n",
    "sdfData = scSpark.read.csv(data_file, header=True, sep=\",\").cache()\n",
    "\n",
    "gender = sdfData.groupBy('Gender').count()\n",
    "print(gender.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL allows you to use SQL like queries to access the data. First, we create a temporary table out of the dataframe. For that purpose registerTampTable is used. In our case the table name is sales. Once it’s done you can use typical SQL queries on it. In our case it is Select * from sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|     Date| Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715| 1/5/2019|13:08|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
      "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22| 3/8/2019|10:29|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
      "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255| 3/3/2019|13:23|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
      "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|1/27/2019|20:33|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
      "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785| 2/8/2019|10:37|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-----+-----------+------+-----------------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# registerTampTable is deprecated \n",
    "sdfData.createOrReplaceTempView(\"sales\")\n",
    "output =  scSpark.sql('SELECT * from sales limit 5')\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+------+-------+---------+-----+-----------+-----+-----------------------+------------+------+\n",
      "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity|Tax 5%|  Total|     Date| Time|    Payment| cogs|gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+------+-------+---------+-----+-----------+-----+-----------------------+------------+------+\n",
      "|351-62-0822|     B| Mandalay|       Member|Female| Fashion accessories|     14.48|       4| 2.896| 60.816| 2/6/2019|18:07|    Ewallet|57.92|            4.761904762|       2.896|   4.5|\n",
      "|871-39-9221|     C|Naypyitaw|       Normal|Female|Electronic access...|     12.45|       6| 3.735| 78.435| 2/9/2019|13:11|       Cash| 74.7|            4.761904762|       3.735|   4.1|\n",
      "|586-25-0848|     A|   Yangon|       Normal|Female|   Sports and travel|     12.34|       7| 4.319| 90.699| 3/4/2019|11:19|Credit card|86.38|            4.761904762|       4.319|   6.7|\n",
      "|389-25-3394|     C|Naypyitaw|       Normal|  Male|Electronic access...|     11.81|       5|2.9525|62.0025|2/17/2019|18:06|       Cash|59.05|            4.761904762|      2.9525|   9.4|\n",
      "|279-62-1445|     C|Naypyitaw|       Member|Female| Fashion accessories|     12.54|       1| 0.627| 13.167|2/21/2019|12:38|       Cash|12.54|            4.761904762|       0.627|   8.2|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+------+-------+---------+-----+-----------+-----+-----------------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = scSpark.sql('SELECT * from sales WHERE `Unit Price` < 15 AND Quantity < 10 limit 5' )\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even aggregated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|total|     City|\n",
      "+-----+---------+\n",
      "|  328|Naypyitaw|\n",
      "|  332| Mandalay|\n",
      "|  340|   Yangon|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = scSpark.sql('SELECT COUNT(*) as total, City from sales GROUP BY City')\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving, Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the LOAD part of the ETL. What if you want to save this transformed data? Well, you have many options available, RDBMS, XML or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.write.format('json').save('filtered.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It created a folder with the name of the file, in our case it is filtered.json. Then, a file with the name _SUCCESStells whether the operation was a success or not. In case it fails a file with the name _FAILURE is generated. Then, you find multiple files here. The reason for multiple files is that each work is involved in the operation of writing in the file. If you want to create a single file(which is not recommended) then coalesce can be used that collects and reduces the data from all partitions to a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work, it saves in the same way as above \n",
    "# output.coalesce(1).write.format('json').save('filtered_single.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output the following data:\n",
    "\n",
    "{\"total\":328,\"City\":\"Naypyitaw\"} <br>\n",
    "{\"total\":332,\"City\":\"Mandalay\"} <br>\n",
    "{\"total\":340,\"City\":\"Yangon\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MySQL and Apache Spark Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dataframe contains the transformed data. We would like to load this data into MYSQL for further usage like Visualization or showing on an app. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need the MySQL connector library to interact with Spark. We will download the connector from MySQL website and put it in a folder. We will amend SparkSession to include the JAR file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' scSpark = SparkSession         .builder         .appName(\"reading csv\")         .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/mysql-connector-java-8.0.16.jar\")         .getOrCreate() '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" scSpark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"reading csv\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/mysql-connector-java-8.0.16.jar\") \\\n",
    "        .getOrCreate() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" output.write.format('jdbc').options(\\n        url='jdbc:mysql://localhost/spark',\\n        driver='com.mysql.cj.jdbc.Driver',\\n        dbtable='city_info',\\n        user='root',\\n        password='root').mode('append').save() \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = scSpark.sql('SELECT COUNT(*) as total, City from sales GROUP BY City')\n",
    "# output.show()\n",
    "\"\"\" output.write.format('jdbc').options(\n",
    "        url='jdbc:mysql://localhost/spark',\n",
    "        driver='com.mysql.cj.jdbc.Driver',\n",
    "        dbtable='city_info',\n",
    "        user='root',\n",
    "        password='root').mode('append').save() \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bbaf081f8b3bca6151094b17c56870e42b6483d70c4ebda089a89290f7fe980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
