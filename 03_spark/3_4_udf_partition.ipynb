{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "# Glue Studio Notebook\n",
    "You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
    "\n",
    "## Available Magics\n",
    "|          Magic              |   Type       |                                                                        Description                                                                        |\n",
    "|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
    "| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
    "| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
    "| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n",
    "| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
    "| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
    "| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
    "| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
    "| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
    "| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
    "| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n",
    "| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n",
    "| %security_configuration     |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
    "| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
    "| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
    "| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n",
    "| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
    "| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
    "| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "It looks like there is a newer version of the kernel available. The latest version is 0.32 and you have 0.30 installed.\n",
      "Please run `pip install --upgrade aws-glue-sessions` to upgrade your kernel\n",
      "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::850188643689:role/gluenotebook_role\n",
      "Attempting to use existing AssumeRole session credentials.\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 5\n",
      "Session ID: f79d85e6-de39-44e2-9566-b5fd2c14a21d\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.30\n",
      "--enable-glue-datacatalog true\n",
      "Waiting for session f79d85e6-de39-44e2-9566-b5fd2c14a21d to get into ready status...\n",
      "Session f79d85e6-de39-44e2-9566-b5fd2c14a21d has been created\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already connected to session c324f95f-a113-471c-8d0d-71a9258458af. Your change will not reflect in the current session, but it will affect future new sessions. \n",
      "\n",
      "Current idle_timeout is None minutes.\n",
      "idle_timeout has been set to 180 minutes.\n"
     ]
    }
   ],
   "source": [
    "%idle_timeout 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take all A* companies from S3\n",
    "2. Use the time magic cell: %%time Now all your operations will be measured\n",
    "3. Create a new column Month, based on Date using regexp\n",
    "4. Create a second column Month2, based on Date, but using UDF\n",
    "5. Compare the performance\n",
    "\n",
    "https://drive.google.com/file/d/1xuN9twPlaz_Y9pOyG3GfnohvH_PnReVC/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"AWSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Take all A* companies from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv(\"s3a://aws-stocks-dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"stock_index\", input_file_name())\n",
    "df = df.withColumn('stock_index', regexp_replace('stock_index', 's3a://aws-stocks-dataset/', ''))\n",
    "df = df.withColumn('stock_index', regexp_replace('stock_index', '.csv', '')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+------+-------------------+-------------------+-------------------+-----------+\n",
      "|      Date|                Low|Open|Volume|               High|              Close|     Adjusted Close|stock_index|\n",
      "+----------+-------------------+----+------+-------------------+-------------------+-------------------+-----------+\n",
      "|21-02-1973|0.39506199955940247| 0.0| 15188|0.39506199955940247|0.39506199955940247|0.39506199955940247|       DIOD|\n",
      "|22-02-1973| 0.3703700006008148| 0.0|  9113| 0.3703700006008148| 0.3703700006008148| 0.3703700006008148|       DIOD|\n",
      "+----------+-------------------+----+------+-------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df = df.filter(df.stock_index.startswith(\"A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+\n",
      "|      Date|                Low|               Open|   Volume|               High|              Close|     Adjusted Close|stock_index|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+\n",
      "|12-12-1980| 0.1283479928970337| 0.1283479928970337|469033600| 0.1289059966802597| 0.1283479928970337|0.10003948211669922|       AAPL|\n",
      "|15-12-1980|0.12165199965238571|0.12221000343561172|175884800|0.12221000343561172|0.12165199965238571|0.09482034295797348|       AAPL|\n",
      "|16-12-1980|0.11272300034761429| 0.1132809966802597|105728000| 0.1132809966802597|0.11272300034761429|0.08786075562238693|       AAPL|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.15 µs\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+\n",
      "|      Date|                Low|               Open|   Volume|               High|              Close|     Adjusted Close|stock_index|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+\n",
      "|12-12-1980| 0.1283479928970337| 0.1283479928970337|469033600| 0.1289059966802597| 0.1283479928970337|0.10003948211669922|       AAPL|\n",
      "|15-12-1980|0.12165199965238571|0.12221000343561172|175884800|0.12221000343561172|0.12165199965238571|0.09482034295797348|       AAPL|\n",
      "|16-12-1980|0.11272300034761429| 0.1132809966802597|105728000| 0.1132809966802597|0.11272300034761429|0.08786075562238693|       AAPL|\n",
      "|17-12-1980|0.11551299691200256|0.11551299691200256| 86441600|0.11607100069522858|0.11551299691200256|0.09003540128469467|       AAPL|\n",
      "|18-12-1980|0.11886200308799744|0.11886200308799744| 73449600|0.11941999942064285|0.11886200308799744|0.09264572709798813|       AAPL|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df.createOrReplaceTempView(\"stocks\")\n",
    "query = \"SELECT * FROM stocks WHERE stock_index LIKE 'A%'\"\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a new column Month, based on Date using regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.48 µs\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+-----+\n",
      "|      Date|                Low|               Open|   Volume|               High|              Close|     Adjusted Close|stock_index|Month|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+-----+\n",
      "|12-12-1980| 0.1283479928970337| 0.1283479928970337|469033600| 0.1289059966802597| 0.1283479928970337|0.10003948211669922|       AAPL|   12|\n",
      "|15-12-1980|0.12165199965238571|0.12221000343561172|175884800|0.12221000343561172|0.12165199965238571|0.09482034295797348|       AAPL|   12|\n",
      "|16-12-1980|0.11272300034761429| 0.1132809966802597|105728000| 0.1132809966802597|0.11272300034761429|0.08786075562238693|       AAPL|   12|\n",
      "|17-12-1980|0.11551299691200256|0.11551299691200256| 86441600|0.11607100069522858|0.11551299691200256|0.09003540128469467|       AAPL|   12|\n",
      "|18-12-1980|0.11886200308799744|0.11886200308799744| 73449600|0.11941999942064285|0.11886200308799744|0.09264572709798813|       AAPL|   12|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df = df.withColumn(\"Month\", substring(\"Date\", 4, 2))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a second column Month2, based on Date, but using UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/ \n",
    "def extractMonth(date_string):\n",
    "    month = date_string.split('-')[1]\n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "udf_extractMonth = udf(extractMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('Month2',  udf_extractMonth('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.48 µs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def extractMonth(date_string):\n",
    "    month = date_string.split('-')[1]\n",
    "    return month\n",
    "\n",
    "udf_extractMonth = udf(extractMonth)\n",
    "\n",
    "df = df.withColumn('Month2',  udf_extractMonth('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+-----+------+\n",
      "|      Date|                Low|               Open|   Volume|               High|              Close|     Adjusted Close|stock_index|Month|Month2|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+-----+------+\n",
      "|12-12-1980| 0.1283479928970337| 0.1283479928970337|469033600| 0.1289059966802597| 0.1283479928970337|0.10003948211669922|       AAPL|   12|    12|\n",
      "|15-12-1980|0.12165199965238571|0.12221000343561172|175884800|0.12221000343561172|0.12165199965238571|0.09482034295797348|       AAPL|   12|    12|\n",
      "|16-12-1980|0.11272300034761429| 0.1132809966802597|105728000| 0.1132809966802597|0.11272300034761429|0.08786075562238693|       AAPL|   12|    12|\n",
      "|17-12-1980|0.11551299691200256|0.11551299691200256| 86441600|0.11607100069522858|0.11551299691200256|0.09003540128469467|       AAPL|   12|    12|\n",
      "|18-12-1980|0.11886200308799744|0.11886200308799744| 73449600|0.11941999942064285|0.11886200308799744|0.09264572709798813|       AAPL|   12|    12|\n",
      "+----------+-------------------+-------------------+---------+-------------------+-------------------+-------------------+-----------+-----+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read 100k rows from aws-stocks-dataset S3 Bucket and save them locally to improve performance (or take already existing local file)\n",
    "2. Get number of Spark partitions with rdd.getNumPartitions\n",
    "2. Get max \"high\" per partition: .withColumn(\"partition\", spark_partition_id()) + group by\n",
    "3. Repartition the dataset by field stock_index\n",
    "5. Review the dataset\n",
    "6. Use Glue write_dynamic_frame with partitionKeys option set to Date, to write data to your you OWN folder to s3 bucket aws-stocks-dataset-output. It should be partitioned on the folders level by date\n",
    "7. Try to filter data from s3 using Date filter and other fields. Check the performance difference with %%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read 100k rows from aws-stocks-dataset S3 Bucket and save them locally to improve performance (or take already existing local file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# AnalysisException: 'Attribute name \"Adjusted Close\" contains invalid character(s) among \" ,;{}()\\\\n\\\\t=\". Please use alias to rename it.;'\n",
    "df = df.withColumnRenamed(\"Adjusted Close\", \"Adjusted_close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted_close', 'stock_index', 'Month', 'Month2']\n"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.write.parquet(\"/media/ubi20/SanDisk/code/data_engineer/03_module_etl2_spark/stock.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.csv(\"/home/Downloads/stock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# df.write.option(\"header\",True).mode(\"overwrite\").csv(\"home/Downloads/stock.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get number of Spark partitions with rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get max \"high\" per partition: .withColumn(\"partition\", spark_partition_id()) + group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"partition\", spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+-----------------+-----------+-----+------+---------+\n",
      "|      Date|   Low|  Open|Volume|  High| Close|   Adjusted_close|stock_index|Month|Month2|partition|\n",
      "+----------+------+------+------+------+------+-----------------+-----------+-----+------+---------+\n",
      "|03-05-1973| 3.625| 3.625|  2000|3.8125| 3.625|2.070335865020752|       ALCO|   05|    05|        0|\n",
      "|04-05-1973| 3.625| 3.625|  1600|3.8125| 3.625|2.070335865020752|       ALCO|   05|    05|        0|\n",
      "|07-05-1973| 3.625| 3.625|  1600|3.8125| 3.625|2.070335865020752|       ALCO|   05|    05|        0|\n",
      "|08-05-1973|3.6875|3.6875|  1600| 3.875|3.6875|2.106030225753784|       ALCO|   05|    05|        0|\n",
      "|09-05-1973|3.6875|3.6875|  6000| 3.875|3.6875|2.106030225753784|       ALCO|   05|    05|        0|\n",
      "+----------+------+------+------+------+------+-----------------+-----------+-----+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adjusted_close: string (nullable = true)\n",
      " |-- stock_index: string (nullable = false)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Month2: string (nullable = true)\n",
      " |-- partition: integer (nullable = false)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.types import DoubleType\n",
    "#df = df.withColumn(\"High\",col(\"High\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"High_int\",col(\"High\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|partition|    max_high|\n",
      "+---------+------------+\n",
      "|        0|99943.203125|\n",
      "+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"partition\").agg(max(\"High\").alias(\"max_high\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Repartition the dataset by field stock_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "df_reparted = df.repartition(5, \"stock_index\")\n",
    "df_reparted.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Review the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_reparted = df_reparted.withColumn(\"partition\", spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n",
      "+---------+-----------------+\n",
      "|partition|         max_high|\n",
      "+---------+-----------------+\n",
      "|        1|99.98999786376953|\n",
      "|        3|9.997777938842773|\n",
      "|        2|99.30000305175781|\n",
      "|        4|9.989999771118164|\n",
      "|        0| 99.9000015258789|\n",
      "+---------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "df_reparted.groupBy(\"partition\").agg(max(\"High\").alias(\"max_high\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df_reparted.select('partition').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df_reparted.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use Glue write_dynamic_frame with partitionKeys option set to Date, to write data to your you OWN folder to s3 bucket aws-stocks-dataset-output. It should be partitioned on the folders level by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_reparted_date = df.repartition(50, \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_reparted_date = df_reparted_date.withColumn(\"partition\", spark_partition_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------+-------------------+--------------------+-------------------+-----------+-----+------+---------+--------+\n",
      "|      Date|                 Low|                Open|  Volume|               High|               Close|     Adjusted_close|stock_index|Month|Month2|partition|High_int|\n",
      "+----------+--------------------+--------------------+--------+-------------------+--------------------+-------------------+-----------+-----+------+---------+--------+\n",
      "|19-01-1981| 0.14676299691200256| 0.14676299691200256|41574400|0.14732100069522858| 0.14676299691200256|0.11439288407564163|       AAPL|   01|    01|        0|       0|\n",
      "|10-03-1981| 0.10044600069522858| 0.10100399702787399|28380800|0.10100399702787399| 0.10044600069522858|0.07829158008098602|       AAPL|   03|    03|        0|       0|\n",
      "|26-08-1981| 0.08482100069522858| 0.08537899702787399|33600000|0.08537899702787399| 0.08482100069522858|0.06611282378435135|       AAPL|   08|    08|        0|       0|\n",
      "|02-02-1982| 0.09040199965238571| 0.09040199965238571|54275200|0.09151799976825714| 0.09040199965238571|0.07046288251876831|       AAPL|   02|    02|        0|       0|\n",
      "|14-06-1982|0.059709999710321426|0.059709999710321426|29993600|0.06026799976825714|0.059709999710321426|0.04654031619429588|       AAPL|   06|    06|        0|       0|\n",
      "+----------+--------------------+--------------------+--------+-------------------+--------------------+-------------------+-----------+-----+------+---------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_reparted_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "df_reparted_date.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = DynamicFrame.fromDF(df_reparted_date.limit(10), glueContext, \"ddf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glueContext.write_dynamic_frame.from_options(ddf, connection_type=\"s3\", connection_options={\"path\": \"s3://aws-stocks-dataset-output/attila/stocks\", \"partitionKeys\": [\"Date\"]}, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Try to filter data from s3 using Date filter and other fields. Check the performance difference with %%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with nested data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the JSON data from S3 Bucket s3://spark-concerts-json/zipcodes.json\n",
    "2. Print schema and make sure that it's inferred correctly\n",
    "3. Unnest operator field and get normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c57434cc3cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://spark-concerts-json/zipcodes.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"s3://spark-concerts-json/zipcodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception encountered while retrieving session: An error occurred (ExpiredTokenException) when calling the GetSession operation: The security token included in the request is expired \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/aws_glue_interactive_sessions_kernel/glue_pyspark/GlueKernel.py\", line 688, in get_current_session\n",
      "    current_session = self.glue_client.get_session(Id=self.get_session_id())[\"Session\"]\n",
      "  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/botocore/client.py\", line 415, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/botocore/client.py\", line 745, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (ExpiredTokenException) when calling the GetSession operation: The security token included in the request is expired\n",
      "Failed to retrieve session status \n",
      "Exception encountered while running statement: An error occurred (ExpiredTokenException) when calling the RunStatement operation: The security token included in the request is expired \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/aws_glue_interactive_sessions_kernel/glue_pyspark/GlueKernel.py\", line 121, in do_execute\n",
      "    statement_id = self.glue_client.run_statement(SessionId=self.get_session_id(), Code=code)[\"Id\"]\n",
      "  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/botocore/client.py\", line 415, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/home/jupyter-user/.local/lib/python3.7/site-packages/botocore/client.py\", line 745, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (ExpiredTokenException) when calling the RunStatement operation: The security token included in the request is expired\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/\n",
    "df.select('operator.name')\n",
    "# df_name = df.select('operator.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.withColumn(\"name\", col(\"operator\").getField(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bbaf081f8b3bca6151094b17c56870e42b6483d70c4ebda089a89290f7fe980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
